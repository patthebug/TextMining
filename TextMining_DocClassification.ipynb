{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import re\n",
      "import csv\n",
      "from nltk.stem.snowball import SnowballStemmer\n",
      "from sklearn.pipeline import Pipeline\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.corpus import wordnet\n",
      "\n",
      "# this path will need to be changed in order to run the program\n",
      "os.chdir('/home/patthebug/amazon/amazon')\n",
      "\n",
      "amazon_reviews = []\n",
      "target_labels = []\n",
      "\n",
      "# read the reviews and store them in a list\n",
      "for infile in os.listdir(os.path.join(os.getcwd())):\n",
      "    if infile.endswith('csv'):\n",
      "        label = infile.split('.')[0]\n",
      "        target_labels.append(label)\n",
      "        \n",
      "        with open(infile, 'rb') as csvfile:\n",
      "            amazon_reader = csv.DictReader(csvfile, delimiter=',')\n",
      "            infile_rows = [{ label: row['review_text'] } for row in amazon_reader]\n",
      "            \n",
      "        for doc in infile_rows:\n",
      "            amazon_reviews.append(doc)\n",
      "        \n",
      "print 'There are ' + str(len(amazon_reviews)) + ' total reviews.'\n",
      "\n",
      "print 'The labels are '+ ', '.join(target_labels) + '.'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 5522 total reviews.\n",
        "The labels are literature_2010, sociology_2010, biologicalsciences_2010.\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# I am using snowball stemmer as its performance is better than Porter stemmer\n",
      "stemmer = SnowballStemmer('english')\n",
      "\n",
      "def stem_tokens(tokens, stemmer):\n",
      "    stemmed = []\n",
      "    for item in tokens:\n",
      "        if len(item) <= 2:\n",
      "            continue\n",
      "        else:\n",
      "            if(wordnet.synsets(item)):\n",
      "                stemmed.append(stemmer.stem(item))\n",
      "    return stemmed\n",
      "\n",
      "def tokenize(text):\n",
      "    # only accept words that beegin with an alphabet\n",
      "    tokenizer = RegexpTokenizer('[A-Za-z]\\w+')\n",
      "    tokens = tokenizer.tokenize(text)\n",
      "    stems = stem_tokens(tokens, stemmer)\n",
      "    return stems\n",
      "\n",
      "lemmatizer = WordNetLemmatizer()\n",
      "\n",
      "def stem_tokens_lemmatize(tokens, stemmer):\n",
      "    stemmed = []\n",
      "    for item in tokens:\n",
      "        stemmed.append(lemmatizer.lemmatize(item))\n",
      "    return stemmed\n",
      "\n",
      "def tokenize_lemmatize(text):\n",
      "    # only accept words that beegin with an alphabet\n",
      "    tokenizer = RegexpTokenizer('[A-Za-z]\\w+')\n",
      "    tokens = tokenizer.tokenize(text)\n",
      "    stems = stem_tokens_lemmatize(tokens, stemmer)\n",
      "    return stems"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#first, we need to shuffle the docs into random order\n",
      "#this is to make it easier for me to make train and test sets\n",
      "\n",
      "from random import shuffle\n",
      "x = [amazon_reviews[i] for i in range(len(amazon_reviews))]\n",
      "shuffle(x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn import metrics\n",
      "from operator import itemgetter\n",
      "import numpy as np\n",
      "\n",
      "trainset_size = int(round(len(amazon_reviews)*0.75)) # i chose this threshold arbitrarily...\n",
      "print 'The training set size for this classifier is ' + str(trainset_size) + '\\n'\n",
      "\n",
      "training_X = np.array([''.join(el.values()) for el in x[0:trainset_size]])\n",
      "training_y = np.array([''.join(el.keys()) for el in x[0:trainset_size]])\n",
      "\n",
      "testing_X = np.array([''.join(el.values()) for el in x[trainset_size+1:len(amazon_reviews)]])   \n",
      "testing_y = np.array([''.join(el.keys()) for el in x[trainset_size+1:len(amazon_reviews)]])  \n",
      "\n",
      "# vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1, 1), stop_words='english', strip_accents='unicode', norm='l2')\n",
      "\n",
      "# create the pipeline for vectorizer\n",
      "pipeline = Pipeline((\n",
      "    ('vec', TfidfVectorizer(min_df=1, stop_words='english', use_idf=True,strip_accents='unicode', ngram_range=(1,1), tokenizer=tokenize, norm='l2')),\n",
      "    ('clf', MultinomialNB()),\n",
      "))\n",
      "\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "# vary various paramters to be passed into the pipeline\n",
      "parameters = {\n",
      "    'vec__tokenizer': [tokenize, tokenize_lemmatize],\n",
      "    'vec__min_df': [0.0, 0.05, 0.1],\n",
      "    'clf__alpha': [0.001, 0.01, 0.1, 0.2],\n",
      "}\n",
      "\n",
      "# apply gridsearch here, the parameter n_jobs=-1 is very important as it distributes the jobs amongst various cores of the processor\n",
      "# thus speeding up the process\n",
      "gs = GridSearchCV(pipeline, parameters, verbose=2, refit=False, n_jobs=-1)\n",
      "_ = gs.fit(training_X, training_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The training set size for this classifier is 4142\n",
        "\n",
        "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:   49.3s\n",
        "[Parallel(n_jobs=-1)]: Done  41 jobs       | elapsed:  5.3min\n",
        "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed: 10.0min finished\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.0, clf__alpha=0.001 \n",
        "[GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.0, clf__alpha=0.001 \n",
        "[GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.0, clf__alpha=0.001 \n",
        "[GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.0, clf__alpha=0.001 \n",
        "[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.0, clf__alpha=0.001 -  49.2s[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.0, clf__alpha=0.001 -  50.0s[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.0, clf__alpha=0.001 -  50.6s[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.0, clf__alpha=0.001 -  12.4s\n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.05, clf__alpha=0.001 [GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.05, clf__alpha=0.001 [GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.05, clf__alpha=0.001 [GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.0, clf__alpha=0.001 \n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.05, clf__alpha=0.001 -  46.8s[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.05, clf__alpha=0.001 -  47.3s[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.05, clf__alpha=0.001 -  12.3s[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.0, clf__alpha=0.001 -   9.9s\n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.1, clf__alpha=0.001 [GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.1, clf__alpha=0.001 [GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.05, clf__alpha=0.001 [GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.0, clf__alpha=0.001 \n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.1, clf__alpha=0.001 -  49.3s[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.1, clf__alpha=0.001 -  13.0s[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.05, clf__alpha=0.001 -  12.2s[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.0, clf__alpha=0.001 -  10.8s\n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.0, clf__alpha=0.01 [GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.1, clf__alpha=0.001 [GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.05, clf__alpha=0.001 [GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.05, clf__alpha=0.001 \n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.0, clf__alpha=0.01 -  13.2s[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.1, clf__alpha=0.001 -  12.7s[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.05, clf__alpha=0.001 -  12.5s[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.05, clf__alpha=0.001 -  53.2s\n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.0, clf__alpha=0.01 [GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.1, clf__alpha=0.001 [GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.1, clf__alpha=0.001 [GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.1, clf__alpha=0.001 \n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.0, clf__alpha=0.01 -  13.3s[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.1, clf__alpha=0.001 -  12.9s[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.1, clf__alpha=0.001 -  48.8s[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.1, clf__alpha=0.001 -  49.9s\n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.0, clf__alpha=0.01 [GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.0, clf__alpha=0.01 [GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.0, clf__alpha=0.01 [GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.0, clf__alpha=0.01 \n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.0, clf__alpha=0.01 -  13.3s[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.0, clf__alpha=0.01 -  50.0s[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.0, clf__alpha=0.01 -  51.5s[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.0, clf__alpha=0.01 -  50.5s\n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.05, clf__alpha=0.01 [GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.05, clf__alpha=0.01 [GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.05, clf__alpha=0.01 [GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.05, clf__alpha=0.01 \n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.05, clf__alpha=0.01 -  53.2s[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.05, clf__alpha=0.01 -  52.8s[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.05, clf__alpha=0.01 -  14.2s[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.05, clf__alpha=0.01 -  52.4s\n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.1, clf__alpha=0.01 [GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.1, clf__alpha=0.01 [GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.05, clf__alpha=0.01 [GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.1, clf__alpha=0.01 \n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.1, clf__alpha=0.01 -  52.7s[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.1, clf__alpha=0.01 -  52.1s[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.05, clf__alpha=0.01 -  14.0s[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.1, clf__alpha=0.01 -  13.7s\n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.0, clf__alpha=0.1 [GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.0, clf__alpha=0.1 [GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.05, clf__alpha=0.01 [GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.1, clf__alpha=0.01 \n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.0, clf__alpha=0.1 -  13.6s[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.0, clf__alpha=0.1 -  53.2s[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.05, clf__alpha=0.01 -  13.9s[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.1, clf__alpha=0.01 -  13.6s\n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.0, clf__alpha=0.1 [GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.05, clf__alpha=0.1 [GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.1, clf__alpha=0.01 [GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.1, clf__alpha=0.01 \n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.0, clf__alpha=0.1 -  13.8s[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.05, clf__alpha=0.1 -  13.7s[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.1, clf__alpha=0.01 -  52.8s[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.1, clf__alpha=0.01 -  13.6s\n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.0, clf__alpha=0.1 [GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.05, clf__alpha=0.1 [GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.0, clf__alpha=0.1 [GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.0, clf__alpha=0.1 \n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.0, clf__alpha=0.1 -  14.6s[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.05, clf__alpha=0.1 -  13.7s[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.0, clf__alpha=0.1 -  52.9s[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.0, clf__alpha=0.1 -  52.1s\n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.05, clf__alpha=0.1 [GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.05, clf__alpha=0.1 [GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.05, clf__alpha=0.1 [GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.05, clf__alpha=0.1 \n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.05, clf__alpha=0.1 -  53.9s[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.05, clf__alpha=0.1 -  14.7s[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.05, clf__alpha=0.1 -  54.2s[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.05, clf__alpha=0.1 -  54.2s\n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.1, clf__alpha=0.1 [GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.1, clf__alpha=0.1 [GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.1, clf__alpha=0.1 [GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.1, clf__alpha=0.1 \n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.1, clf__alpha=0.1 -  55.1s[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.1, clf__alpha=0.1 -  56.2s[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.1, clf__alpha=0.1 -  16.3s[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.1, clf__alpha=0.1 -  55.4s\n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.0, clf__alpha=0.2 [GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.0, clf__alpha=0.2 [GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.1, clf__alpha=0.1 [GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.0, clf__alpha=0.2 \n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.0, clf__alpha=0.2 -  55.6s[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.0, clf__alpha=0.2 -  14.0s[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.1, clf__alpha=0.1 -  14.0s[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.0, clf__alpha=0.2 -  54.7s\n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.05, clf__alpha=0.2 [GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.0, clf__alpha=0.2 [GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.1, clf__alpha=0.1 [GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.05, clf__alpha=0.2 \n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.05, clf__alpha=0.2 -  15.3s[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.0, clf__alpha=0.2 -  14.1s[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.1, clf__alpha=0.1 -  13.6s[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.05, clf__alpha=0.2 -  56.6s\n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.05, clf__alpha=0.2 [GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.0, clf__alpha=0.2 [GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.0, clf__alpha=0.2 [GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.1, clf__alpha=0.2 \n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.05, clf__alpha=0.2 -  15.2s[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.0, clf__alpha=0.2 -  14.6s[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.0, clf__alpha=0.2 -  53.6s[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.1, clf__alpha=0.2 -  15.0s\n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.05, clf__alpha=0.2 [GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.05, clf__alpha=0.2 [GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.05, clf__alpha=0.2 [GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.1, clf__alpha=0.2 "
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'The best score is: ' + str(gs.best_score_) + '\\n'\n",
      "print 'The best parameters are: ' + str(gs.best_params_) + '\\n'\n",
      "# watch the change in scores over different iterations\n",
      "print 'Grid scores are: ' + str(gs.grid_scores_)\n",
      "# looking at all the scores from different iterations, it seems like scores are lower wherever the value of alpha is extremely low (0.001) \n",
      "# or on the higher side (0.1) and the value of min_df is high. The best scores are received the min_df is 0 and alpha is 0.01"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The best score is: 0.929985514244\n",
        "\n",
        "The best parameters are: {'vec__tokenizer': <function tokenize_lemmatize at 0x3c10050>, 'vec__min_df': 0.0, 'clf__alpha': 0.01}\n",
        "\n",
        "Grid scores are: [mean: 0.90464, std: 0.00485, params: {'vec__tokenizer': <function tokenize at 0x3aeae60>, 'vec__min_df': 0.0, 'clf__alpha': 0.001}, mean: 0.91284, std: 0.00306, params: {'vec__tokenizer': <function tokenize_lemmatize at 0x3c10050>, 'vec__min_df': 0.0, 'clf__alpha': 0.001}, mean: 0.83945, std: 0.00439, params: {'vec__tokenizer': <function tokenize at 0x3aeae60>, 'vec__min_df': 0.05, 'clf__alpha': 0.001}, mean: 0.82448, std: 0.01312, params: {'vec__tokenizer': <function tokenize_lemmatize at 0x3c10050>, 'vec__min_df': 0.05, 'clf__alpha': 0.001}, mean: 0.79527, std: 0.00027, params: {'vec__tokenizer': <function tokenize at 0x3aeae60>, 'vec__min_df': 0.1, 'clf__alpha': 0.001}, mean: 0.79527, std: 0.00027, params: {'vec__tokenizer': <function tokenize_lemmatize at 0x3c10050>, 'vec__min_df': 0.1, 'clf__alpha': 0.001}, mean: 0.91888, std: 0.00416, params: {'vec__tokenizer': <function tokenize at 0x3aeae60>, 'vec__min_df': 0.0, 'clf__alpha': 0.01}, mean: 0.92999, std: 0.00383, params: {'vec__tokenizer': <function tokenize_lemmatize at 0x3c10050>, 'vec__min_df': 0.0, 'clf__alpha': 0.01}, mean: 0.83945, std: 0.00439, params: {'vec__tokenizer': <function tokenize at 0x3aeae60>, 'vec__min_df': 0.05, 'clf__alpha': 0.01}, mean: 0.82448, std: 0.01312, params: {'vec__tokenizer': <function tokenize_lemmatize at 0x3c10050>, 'vec__min_df': 0.05, 'clf__alpha': 0.01}, mean: 0.79527, std: 0.00027, params: {'vec__tokenizer': <function tokenize at 0x3aeae60>, 'vec__min_df': 0.1, 'clf__alpha': 0.01}, mean: 0.79527, std: 0.00027, params: {'vec__tokenizer': <function tokenize_lemmatize at 0x3c10050>, 'vec__min_df': 0.1, 'clf__alpha': 0.01}, mean: 0.91453, std: 0.00216, params: {'vec__tokenizer': <function tokenize at 0x3aeae60>, 'vec__min_df': 0.0, 'clf__alpha': 0.1}, mean: 0.90946, std: 0.00214, params: {'vec__tokenizer': <function tokenize_lemmatize at 0x3c10050>, 'vec__min_df': 0.0, 'clf__alpha': 0.1}, mean: 0.83945, std: 0.00498, params: {'vec__tokenizer': <function tokenize at 0x3aeae60>, 'vec__min_df': 0.05, 'clf__alpha': 0.1}, mean: 0.82400, std: 0.01339, params: {'vec__tokenizer': <function tokenize_lemmatize at 0x3c10050>, 'vec__min_df': 0.05, 'clf__alpha': 0.1}, mean: 0.79527, std: 0.00027, params: {'vec__tokenizer': <function tokenize at 0x3aeae60>, 'vec__min_df': 0.1, 'clf__alpha': 0.1}, mean: 0.79527, std: 0.00027, params: {'vec__tokenizer': <function tokenize_lemmatize at 0x3c10050>, 'vec__min_df': 0.1, 'clf__alpha': 0.1}, mean: 0.89063, std: 0.00311, params: {'vec__tokenizer': <function tokenize at 0x3aeae60>, 'vec__min_df': 0.0, 'clf__alpha': 0.2}, mean: 0.88484, std: 0.00427, params: {'vec__tokenizer': <function tokenize_lemmatize at 0x3c10050>, 'vec__min_df': 0.0, 'clf__alpha': 0.2}, mean: 0.83969, std: 0.00469, params: {'vec__tokenizer': <function tokenize at 0x3aeae60>, 'vec__min_df': 0.05, 'clf__alpha': 0.2}, mean: 0.82327, std: 0.01339, params: {'vec__tokenizer': <function tokenize_lemmatize at 0x3c10050>, 'vec__min_df': 0.05, 'clf__alpha': 0.2}, mean: 0.79527, std: 0.00027, params: {'vec__tokenizer': <function tokenize at 0x3aeae60>, 'vec__min_df': 0.1, 'clf__alpha': 0.2}, mean: 0.79527, std: 0.00027, params: {'vec__tokenizer': <function tokenize_lemmatize at 0x3c10050>, 'vec__min_df': 0.1, 'clf__alpha': 0.2}]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.05, clf__alpha=0.2 -  15.0s[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.05, clf__alpha=0.2 -  57.5s[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.05, clf__alpha=0.2 -  58.5s[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.1, clf__alpha=0.2 -  14.6s\n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.1, clf__alpha=0.2 [GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.1, clf__alpha=0.2 [GridSearchCV] vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.1, clf__alpha=0.2 [GridSearchCV] vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.1, clf__alpha=0.2 \n",
        "\n",
        "\n",
        "\n",
        "[GridSearchCV]  vec__tokenizer=<function tokenize_lemmatize at 0x3c10050>, vec__min_df=0.1, clf__alpha=0.2 -  14.7s[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.1, clf__alpha=0.2 -  54.4s[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.1, clf__alpha=0.2 -  54.3s[GridSearchCV]  vec__tokenizer=<function tokenize at 0x3aeae60>, vec__min_df=0.1, clf__alpha=0.2 -  52.6s\n",
        "\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trainset_size = int(round(len(amazon_reviews)*0.75)) # i chose this threshold arbitrarily...\n",
      "print 'The training set size for this classifier is ' + str(trainset_size) + '\\n'\n",
      "\n",
      "X_train = np.array([''.join(el.values()) for el in x[0:trainset_size]])\n",
      "y_train = np.array([''.join(el.keys()) for el in x[0:trainset_size]])\n",
      "\n",
      "X_test = np.array([''.join(el.values()) for el in x[trainset_size+1:len(amazon_reviews)]])   \n",
      "y_test = np.array([''.join(el.keys()) for el in x[trainset_size+1:len(amazon_reviews)]]) \n",
      "\n",
      "# use the best parameters calculated earlier, alpha = 0.01, min_df=0.0\n",
      "vectorizer = TfidfVectorizer(min_df=0.0, max_df=0.9, tokenizer=tokenize, ngram_range=(1, 1), stop_words='english', strip_accents='unicode', norm='l2')\n",
      "X_train = vectorizer.fit_transform(X_train)\n",
      "X_test = vectorizer.transform(X_test)\n",
      "\n",
      "classifier = MultinomialNB(alpha=0.01).fit(X_train, y_train)\n",
      "y_predicted = classifier.predict(X_test)\n",
      "\n",
      "print 'The precision for this classifier is ' + str(metrics.precision_score(y_test, y_predicted))\n",
      "print 'The recall for this classifier is ' + str(metrics.recall_score(y_test, y_predicted))\n",
      "print 'The f1 for this classifier is ' + str(metrics.f1_score(y_test, y_predicted))\n",
      "\n",
      "print '\\nHere is the confusion matrix:'\n",
      "print metrics.confusion_matrix(y_test, y_predicted)\n",
      "\n",
      "#What are the top N most predictive features per class?\n",
      "N = 10\n",
      "vocabulary = np.array([t for t, i in sorted(vectorizer.vocabulary_.iteritems(), key=itemgetter(1))])\n",
      "\n",
      "for i, label in enumerate(target_labels):\n",
      "    topN = np.argsort(classifier.coef_[i])[-N:]\n",
      "    print \"\\nThe top %d most informative features for %s: \\n%s\" % (N, label, \" \".join(vocabulary[topN]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The training set size for this classifier is 4142\n",
        "\n",
        "The precision for this classifier is 0.92385110545"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "The recall for this classifier is 0.922407541697\n",
        "The f1 for this classifier is 0.913644987367\n",
        "\n",
        "Here is the confusion matrix:\n",
        "[[ 127   45    3]\n",
        " [   4 1102    0]\n",
        " [   3   52   43]]\n",
        "\n",
        "The top 10 most informative features for literature_2010: \n",
        "recommend women help inform intuit violenc gift read fear book\n",
        "\n",
        "The top 10 most informative features for sociology_2010: \n",
        "veri charact love like time novel stori great read book\n",
        "\n",
        "The top 10 most informative features for biologicalsciences_2010: \n",
        "sticker lerner recommend veri chang relationship read help anger book\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trainset_size = int(round(len(amazon_reviews)*0.75))\n",
      "print 'The training set size for this classifier is ' + str(trainset_size) + '\\n'\n",
      "\n",
      "X_train = np.array([''.join(el.values()) for el in x[0:trainset_size]])\n",
      "y_train = np.array([''.join(el.keys()) for el in x[0:trainset_size]])\n",
      "\n",
      "X_test = np.array([''.join(el.values()) for el in x[trainset_size+1:len(amazon_reviews)]])   \n",
      "y_test = np.array([''.join(el.keys()) for el in x[trainset_size+1:len(amazon_reviews)]]) \n",
      "# use the best parameters again and include bi-grams as well in the analysis specified as ngram_range - (1,2)\n",
      "vectorizer = TfidfVectorizer(min_df=0.0, max_df=0.9, tokenizer=tokenize, ngram_range=(1, 2), stop_words='english', strip_accents='unicode', norm='l2')\n",
      "X_train = vectorizer.fit_transform(X_train)\n",
      "X_test = vectorizer.transform(X_test)\n",
      "\n",
      "classifier = MultinomialNB(alpha=0.01).fit(X_train, y_train)\n",
      "y_predicted = classifier.predict(X_test)\n",
      "\n",
      "print 'The precision for this classifier is ' + str(metrics.precision_score(y_test, y_predicted))\n",
      "print 'The recall for this classifier is ' + str(metrics.recall_score(y_test, y_predicted))\n",
      "print 'The f1 for this classifier is ' + str(metrics.f1_score(y_test, y_predicted))\n",
      "\n",
      "print '\\nHere is the confusion matrix:'\n",
      "print metrics.confusion_matrix(y_test, y_predicted)\n",
      "\n",
      "#What are the top N most predictive features per class?\n",
      "N = 10\n",
      "vocabulary = np.array([t for t, i in sorted(vectorizer.vocabulary_.iteritems(), key=itemgetter(1))])\n",
      "\n",
      "for i, label in enumerate(target_labels):\n",
      "    topN = np.argsort(classifier.coef_[i])[-N:]\n",
      "    print \"\\nThe top %d most informative features for %s: \\n%s\" % (N, label, \" \".join(vocabulary[topN]))\n",
      "    \n",
      "# after running this entire step, it can be seen that inclusion of bi-grams did not make any significant difference in the scores.\n",
      "# this is an indication of bi-grams being not very important in this particular analysis. However, according to the confusion matrix,\n",
      "# the accuracy of 'literature' documents has increased slightly. But at the same time, the accuracy of 'biological Sciences' documents has\n",
      "# decreased slightly. For this particular classification problem, inclusion of bi-grams has little effect on the accuracy of the \n",
      "# classifier and thus it is safe to say that bi-grams can be excluded from this analysis to save on computation time."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The training set size for this classifier is 4142\n",
        "\n",
        "The precision for this classifier is 0.923957225082"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "The recall for this classifier is 0.921682378535\n",
        "The f1 for this classifier is 0.911540220591\n",
        "\n",
        "Here is the confusion matrix:\n",
        "[[ 135   40    0]\n",
        " [   6 1099    1]\n",
        " [   6   55   37]]\n",
        "\n",
        "The top 10 most informative features for literature_2010: \n",
        "women help inform gift fear intuit violenc gift read fear book"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "The top 10 most informative features for sociology_2010: \n",
        "veri charact love like time novel stori great read book\n",
        "\n",
        "The top 10 most informative features for biologicalsciences_2010: \n",
        "sticker lerner recommend veri chang relationship read help anger book\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# problem 2 begins here: The following code finds and prints the index of the given document\n",
      "trainset_size = int(len(amazon_reviews))\n",
      "X_train = np.array([''.join(el.values()) for el in x[0:trainset_size]])\n",
      "y_train = np.array([''.join(el.keys()) for el in x[0:trainset_size]])\n",
      "\n",
      "X_test = np.array([''.join(el.values()) for el in x[trainset_size+1:len(amazon_reviews)]])   \n",
      "y_test = np.array([''.join(el.keys()) for el in x[trainset_size+1:len(amazon_reviews)]]) \n",
      "index = 0\n",
      "# all the apostrophes are preceded with backward slashes to act as escape characters\n",
      "toMatch = 'This is the second Turow novel I\\'ve read and I\\'m hooked.  He\\'s an excellent story teller and really knows how to take a plot through unexpected twists and turns.'\n",
      "for i,el in enumerate(X_train):\n",
      "    if(el.__contains__(toMatch)):\n",
      "        print 'The index of the string is: ' + str(i)\n",
      "        index = i\n",
      "        break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The index of the string is: 1945\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# the following code finds 10 most similar documents to the given document using cosine similarity measure\n",
      "vectorizer = TfidfVectorizer(min_df=0.0, max_df=0.9, tokenizer=tokenize, ngram_range=(1, 1), stop_words='english', strip_accents='unicode', norm='l2')\n",
      "tfidf = vectorizer.fit_transform(X_train)\n",
      "from sklearn.metrics.pairwise import linear_kernel\n",
      "cosine_similarities = linear_kernel(tfidf[index], tfidf).flatten()\n",
      "related_docs_indices = cosine_similarities.argsort()[:-11:-1]\n",
      "print X_train[related_docs_indices]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ \"This is the second Turow novel I've read and I'm hooked.  He's an excellent story teller and really knows how to take a plot through unexpected twists and turns.  This book is solid evidence of his skills and I recommend it heartily.<p>I know how popular Grisham is and I've enjoyed some of his writing, but I'd rate Turow as easily the better writer.  This book has plenty to help me reach that conclusion:  family and business deceit, awkward romantic liasons, legal and personal grudges and jealousies, securities fraud, etc.  Try it, I'll bet you'll like it.\"\n",
        " \"I put off reading this book for some time because it looked like too much work.  I've enjoyed many Grisham novels and wanted to try out some other legal thrillers and see what they were like.  Turow is a good writer, without question, but he lacks, at least in this book, some of the flash or pizzaz I was hoping for after reading others.  This novel is definitely not a page turning thriller, but it is a solid story.<p>Some of the legal complications get hard to follow unless you have at least a minimal grasp of how financial markets work, but basic evil is usually pretty easy to spot so don't let the setting throw you off.  Turow obviously works at his craft with some measure of pride, so I don't count the few words and/or concepts that are over my head get the better of me, the context makes the actions plain enough.<p>So the real question is do I give this book a recommendation?  Well, yes. It's an entertaining mystery/legal setting type book and while it doesn't read with the same speed and ease of some others, it's still worth checking out.\"\n",
        " \"On the back of this paperback edition, one review highlight reads, Ludlum stuffs more surprises into his novels than any other six-pack of thriller writers combined. True. Every 10 to 20 pages is yet another plot twist. At well over 500 pages, that's a few too many.\\t\\tI really tried to like this novel. I enjoy mystery novels and really liked the film Bourne Identity, so I figured I would enjoy this book. Well, it was enjoyable, certainly in the first third, but after a while, it dragged on to an awkward conclusion. This book is jam-packed with plot turns, new character introductions, and lots of repetitive inner-monologue.\\t\\tI was really shocked. With Ludlum's reputation, I really expected a tight thriller. It was anything but. I grew tired of the yet-another-plot-device about 150 pages before the conclusion. Which for me, was the worst part...\\t\\tThe ending is very unfulfilling. The books leads up to a climax, yet... well, I won't ruin it for you, in case you decide to see for yourself. I'll just say this - GIGANTIC loose ends (one almost impossible to believe), which left me wanting. So...\\t\\t I picked up Bourne Supremacy to see if Ludlum begins to tie the loose ends up. After all, I am aware this is the Bourne Trilogy. But... nope. Bourne Supremecy - brand new plot, without trying to clear up the unanswered questions of the first book. I had to stop reading that book for fear of more unfulfilled plot promises.\\t\\tIt reminded me of the sprawling plot twists in the endless X-Files series, which ultimately petered out without conclusion. With so much invested, in this case 535 pages, you've got to give me an ending. Jeez.\\t\\tThe movie, it should be stated, is very different, tossing the entire (main) Carlos-the-Assassin plot, for a very tight, streamlined film that BARELY resembles this book. Which is good for the movie. I haven't seen Supremacy, so I have no comment, but I've heard from friends that it strays even farther from the novels.\\t\\tWait... The movie is BETTER than the book?! Yep. No one's more shocked than me.\"\n",
        " 'Good book had a lot of twists and turns, was really easy to read . . . . . .'\n",
        " \"this was the first ludlum book i read - and i am definetly hooked.  it's great. you just can't put it down.  if you like this, also read other bourne books.\"\n",
        " \"I purchased this book because I enjoyed Presumed Innocent and Innocent by Turow. The Burden of Proof has different characters but the premise is somewhat the same: dead wife, family secrets, and legal thrills. No one does this kind of novel so well!! This author will not appeal to everyone, in spite of the popularity of Presumed Innocent. If you are expecting JUST legal thrills, look elsewhere. If you like excellent writing and complex characters you will love these books. He's a master of character motive and plot twists.... Not an easy read, no action but I could not put this book down.\"\n",
        " 'This is an excellent novel by the great Robert Ludlum.  The author has a way that makes everything really mysterious and keeps the reader intrigued. The characters are all very interesting. This is the story of a man who knows nothing of what he is.  He has lethal skills, but no idea how he got them. It is the story of his journey to find his own idenity. The Bourne Idenity is an awesome book...I give it the  best rating I can (5 stars out of 5 stars).  I suggest you read this novel.'\n",
        " \"This was the first book I read by Robert Ludlum. I can say for sure this won't be the last. Ludlum pulls you along with a string and only lets you know what he wants you to know. I was on the edge of my seat reading this from about page 100 on. The plot was excellent and there was great plot twists.\"\n",
        " 'Turow knows how to through it out there.  I really enjoy legal thrillers.  This one had me.  I usually can figure things out, but this one caught me off guard.  Well done.'\n",
        " 'I had to read it more than once! Hooked me on Ludlum.']\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# the following code takes the new document as input and uses the transform method due to the presence of new words\n",
      "toMatch = ['This is the second Turow novel I\\'ve read and I\\'m hooked.  He\\'s an a-grade writer and really knows how to take a plot through unexpected twists and turns.']\n",
      "newDoc = vectorizer.transform(toMatch)\n",
      "cosine_similarities = linear_kernel(newDoc, tfidf).flatten()\n",
      "related_docs_indices = cosine_similarities.argsort()[:-11:-1]\n",
      "print X_train[related_docs_indices]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ \"This is the second Turow novel I've read and I'm hooked.  He's an excellent story teller and really knows how to take a plot through unexpected twists and turns.  This book is solid evidence of his skills and I recommend it heartily.<p>I know how popular Grisham is and I've enjoyed some of his writing, but I'd rate Turow as easily the better writer.  This book has plenty to help me reach that conclusion:  family and business deceit, awkward romantic liasons, legal and personal grudges and jealousies, securities fraud, etc.  Try it, I'll bet you'll like it.\"\n",
        " 'Good book had a lot of twists and turns, was really easy to read . . . . . .'\n",
        " 'I had to read it more than once! Hooked me on Ludlum.'\n",
        " 'I had first read \\\\The Great Gatsby\\\\\" while still in grade school'\n",
        " 'The Bourne Identity is action packed and has so many twists and turns in the plot. It was really amazing.'\n",
        " \"this was the first ludlum book i read - and i am definetly hooked.  it's great. you just can't put it down.  if you like this, also read other bourne books.\"\n",
        " \"Wow. This novel packs a puch. With it's eccentric plots, twist and turns, this is one of the best books I've ever read. I don't know why some people gave it one star. I love this book!  The movie was a disapointment, but if you want a great book, read this!\"\n",
        " \"Wow. This novel packs a puch. With it's eccentric plots, twist and turns, this is one of the best books I've ever read. I don't know why some people gave it one star. I love this book!  The movie was a disapointment, but if you want a great book, read this!\"\n",
        " 'In 9th grade, they made us read \\\\The Pearl'\n",
        " \"My first &quot;action adventure&quot;!  I'm hooked!  I loved this book\"]\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# apply kmeans to the data with 3 clusters (k=3)\n",
      "from sklearn.cluster import KMeans\n",
      "trainset_size = int(len(amazon_reviews))\n",
      "X_train = np.array([''.join(el.values()) for el in x[0:trainset_size]])\n",
      "y_train = np.array([''.join(el.keys()) for el in x[0:trainset_size]])\n",
      "\n",
      "X_test = np.array([''.join(el.values()) for el in x[trainset_size+1:len(amazon_reviews)]])   \n",
      "y_test = np.array([''.join(el.keys()) for el in x[trainset_size+1:len(amazon_reviews)]]) \n",
      "\n",
      "vectorizer = TfidfVectorizer(min_df=0.0, max_df=0.9, tokenizer=tokenize, ngram_range=(1, 1), stop_words='english', strip_accents='unicode', norm='l2')\n",
      "X_train = vectorizer.fit_transform(X_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# implement the adjusted_ran_score function. The value returned by this function is very low indicating the clustering \n",
      "# is not very representative of the document categories\n",
      "kmeans = KMeans(init='k-means++', n_clusters=3, n_init=10, tol=0.00001)\n",
      "kmeans.fit(X_train)\n",
      "from sklearn import metrics\n",
      "metrics.adjusted_rand_score(y_train, kmeans.predict(X_train))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "0.0020421045305739683"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The following code is copied from text_utils.py\n",
      "\n",
      "import networkx as nx\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "def rescale_arr(arr, amin, amax):\n",
      "    \"\"\"Rescale an array to a new range.\n",
      "\n",
      "    Return a new array whose range of values is (amin, amax).\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    arr : array-like\n",
      "\n",
      "    amin : float\n",
      "      new minimum value\n",
      "\n",
      "    amax : float\n",
      "      new maximum value\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> a = np.arange(5)\n",
      "\n",
      "    >>> rescale_arr(a,3,6)\n",
      "    array([ 3.  ,  3.75,  4.5 ,  5.25,  6.  ])\n",
      "    \"\"\"\n",
      "\n",
      "    # old bounds\n",
      "    m = arr.min()\n",
      "    M = arr.max()\n",
      "    # scale/offset\n",
      "    s = float(amax-amin)/(M-m)\n",
      "    d = amin - s*m\n",
      "\n",
      "    # Apply clip before returning to cut off possible overflows outside the\n",
      "    # intended range due to roundoff error, so that we can absolutely guarantee\n",
      "    # that on output, there are no values > amax or < amin.\n",
      "    return np.clip(s*arr+d,amin,amax)\n",
      "\n",
      "\n",
      "def all_pairs(items):\n",
      "    \"\"\"Make all unique pairs (order doesn't matter)\"\"\"\n",
      "    pairs = []\n",
      "    nitems = len(items)\n",
      "    for i, wi in enumerate(items):\n",
      "        for j in range(i+1, nitems):\n",
      "            pairs.append((wi, items[j]))\n",
      "    return pairs\n",
      "\n",
      "\n",
      "def removal_set(words, query):\n",
      "    \"\"\"Create a set of words for removal for a given query.\"\"\"\n",
      "    rem = set(words.split())\n",
      "    qw = [w.lower()  for w in query.split()]\n",
      "    for w in qw:\n",
      "        rem.add(w)\n",
      "        rem.add('#' + w)\n",
      "        qq = ''.join(qw)\n",
      "        rem.add(qq)\n",
      "        rem.add('#' + qq)\n",
      "    return rem\n",
      "\n",
      "def lines_cleanup(lines, min_length=4, remove = None):\n",
      "    \"\"\"Clean up a list of lowercase strings of text for simple analysis.\n",
      "\n",
      "    Splits on whitespace, removes all 'words' less than `min_length` characters\n",
      "    long, and those in the `remove` set.\n",
      "\n",
      "    Returns a list of strings.\n",
      "    \"\"\"\n",
      "    remove = set(remove) if remove is not None else []\n",
      "    filtered = []\n",
      "    for line in lines:\n",
      "        a = []\n",
      "        for w in line.lower().split():\n",
      "            wnorm = w.rstrip('.,:').replace('[', '').replace(']', '')\n",
      "            if len(wnorm) >= min_length and wnorm not in remove:\n",
      "                a.append(wnorm)\n",
      "        filtered.append(' '.join(a))\n",
      "    return filtered\n",
      "\n",
      "\n",
      "def print_vk(lst):\n",
      "    \"\"\"Print a list of value/key pairs nicely formatted in key/value order.\"\"\"\n",
      "\n",
      "    # Find the longest key: remember, the list has value/key paris, so the key\n",
      "    # is element [1], not [0]\n",
      "    longest_key = max([len(word) for word, count in lst])\n",
      "    # Make a format string out of it\n",
      "    fmt = '%'+str(longest_key)+'s -> %s'\n",
      "    # Do actual printing\n",
      "    for k,v in lst:\n",
      "        print fmt % (k,v)\n",
      "\n",
      "\n",
      "def word_freq(text):\n",
      "    \"\"\"Return a dictionary of word frequencies for the given text.\n",
      "\n",
      "    Input text should be given as an iterable of strings.\"\"\"\n",
      "\n",
      "    freqs = {}\n",
      "    for word in text:\n",
      "        freqs[word] = freqs.get(word, 0) + 1\n",
      "    return freqs\n",
      "\n",
      "\n",
      "def sort_freqs(freqs):\n",
      "    \"\"\"Sort a word frequency histogram represented as a dictionary.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    freqs : dict\n",
      "      A dict with string keys and integer values.\n",
      "\n",
      "    Return\n",
      "    ------\n",
      "    items : list\n",
      "      A list of (count, word) pairs.\n",
      "    \"\"\"\n",
      "    items = freqs.items()\n",
      "    items.sort(key = lambda wc: wc[1])\n",
      "    return items\n",
      "\n",
      "\n",
      "def summarize_freq_hist(freqs, n=10):\n",
      "    \"\"\"Print a simple summary of a word frequencies dictionary.\n",
      "\n",
      "    Paramters\n",
      "    ---------\n",
      "    freqs : dict or list\n",
      "      Word frequencies, represented either as a dict of word->count, or as a\n",
      "      list of count->word pairs.\n",
      "\n",
      "    n : int\n",
      "      The number of least/most frequent words to print.\n",
      "    \"\"\"\n",
      "\n",
      "    items = sort_freqs(freqs) if isinstance(freqs, dict) else freqs\n",
      "    print 'Number of unique words:',len(freqs)\n",
      "    print\n",
      "    print '%d least frequent words:' % n\n",
      "    print_vk(items[:n])\n",
      "    print\n",
      "    print '%d most frequent words:' % n\n",
      "    print_vk(items[-n:])\n",
      "\n",
      "\n",
      "def co_occurrences(lines, words):\n",
      "    \"\"\"Return histogram of co-occurrences of words in a list of lines.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    lines : list\n",
      "      A list of strings considered as 'sentences' to search for co-occurrences.\n",
      "\n",
      "    words : list\n",
      "      A list of words from which all unordered pairs will be constructed and\n",
      "      searched for co-occurrences.\n",
      "    \"\"\"\n",
      "    wpairs = all_pairs(words)\n",
      "\n",
      "    # Now build histogram of co-occurrences\n",
      "    co_occur = {}\n",
      "    for w1, w2 in wpairs:\n",
      "        rx = re.compile('%s .*%s|%s .*%s' % (w1, w2, w2, w1))\n",
      "        co_occur[w1, w2] = sum([1 for line in lines if rx.search(line)])\n",
      "\n",
      "    return co_occur\n",
      "\n",
      "\n",
      "def co_occurrences_graph(word_hist, co_occur, cutoff=0):\n",
      "    \"\"\"Convert a word histogram with co-occurrences to a weighted graph.\n",
      "\n",
      "    Edges are only added if the count is above cutoff.\n",
      "    \"\"\"\n",
      "    g = nx.Graph()\n",
      "    for word, count in word_hist:\n",
      "        g.add_node(word, count=count)\n",
      "    for (w1, w2), count in co_occur.iteritems():\n",
      "        if count<=cutoff:\n",
      "            continue\n",
      "        g.add_edge(w1, w2, weight=count)\n",
      "    return g\n",
      "\n",
      "# Hack: offset the most central node to avoid too much overlap\n",
      "rad0 = 0.3\n",
      "\n",
      "def centrality_layout(wgraph, centrality):\n",
      "    \"\"\"Compute a layout based on centrality.\n",
      "    \"\"\"\n",
      "    # Create a list of centralities, sorted by centrality value\n",
      "    cent = sorted(centrality.items(), key=lambda x:float(x[1]), reverse=True)\n",
      "    nodes = [c[0] for c in cent]\n",
      "    cent  = np.array([float(c[1]) for c in cent])\n",
      "    rad = (cent - cent[0])/(cent[-1]-cent[0])\n",
      "    rad = rescale_arr(rad, rad0, 1)\n",
      "    angles = np.linspace(0, 2*np.pi, len(centrality))\n",
      "    layout = {}\n",
      "    for n, node in enumerate(nodes):\n",
      "        r = rad[n]\n",
      "        th = angles[n]\n",
      "        layout[node] = r*np.cos(th), r*np.sin(th)\n",
      "    return layout\n",
      "\n",
      "\n",
      "def plot_graph(wgraph, pos=None, fig=None, title=None):\n",
      "    \"\"\"Conveniently summarize graph visually\"\"\"\n",
      "\n",
      "    # config parameters\n",
      "    edge_min_width= 3\n",
      "    edge_max_width= 12\n",
      "    label_font = 18\n",
      "    node_font = 22\n",
      "    node_alpha = 0.4\n",
      "    edge_alpha = 0.55\n",
      "    edge_cmap = plt.cm.Spectral\n",
      "\n",
      "    # Create figure\n",
      "    if fig is None:\n",
      "        fig, ax = plt.subplots()\n",
      "    else:\n",
      "        ax = fig.add_subplot(111)\n",
      "    fig.subplots_adjust(0,0,1)\n",
      "\n",
      "    # Plot nodes with size according to count\n",
      "    sizes = []\n",
      "    degrees = []\n",
      "    for n, d in wgraph.nodes_iter(data=True):\n",
      "        sizes.append(d['count'])\n",
      "        degrees.append(wgraph.degree(n))\n",
      "\n",
      "    sizes = rescale_arr(np.array(sizes, dtype=float), 100, 1000)\n",
      "\n",
      "    # Compute layout and label edges according to weight\n",
      "    pos = nx.spring_layout(wgraph) if pos is None else pos\n",
      "    labels = {}\n",
      "    width = []\n",
      "    for n1, n2, d in wgraph.edges_iter(data=True):\n",
      "        w = d['weight']\n",
      "        labels[n1, n2] = w\n",
      "        width.append(w)\n",
      "\n",
      "    width = rescale_arr(np.array(width, dtype=float), edge_min_width, \n",
      "                        edge_max_width)\n",
      "\n",
      "    # Draw\n",
      "    nx.draw_networkx_nodes(wgraph, pos, node_size=sizes, node_color=degrees,\n",
      "                           alpha=node_alpha)\n",
      "    nx.draw_networkx_edges(wgraph, pos, width=width, edge_color=width,\n",
      "                           edge_cmap=edge_cmap, alpha=edge_alpha)\n",
      "    nx.draw_networkx_edge_labels(wgraph, pos, edge_labels=labels, \n",
      "                                 font_size=label_font)\n",
      "    nx.draw_networkx_labels(wgraph, pos, font_size=node_font, font_weight='bold')\n",
      "    if title is not None:\n",
      "        ax.set_title(title, fontsize=label_font)\n",
      "    ax.set_xticks([])\n",
      "    ax.set_yticks([])\n",
      "\n",
      "    # Mark centrality axes\n",
      "    kw = dict(color='k', linestyle='-')\n",
      "    cross = [ax.axhline(0, **kw), ax.axvline(rad0, **kw)]\n",
      "    [ l.set_zorder(0) for l in cross]\n",
      "\n",
      "\n",
      "def plot_word_histogram(freqs, show=10, title=None):\n",
      "    \"\"\"Plot a histogram of word frequencies, limited to the top `show` ones.\n",
      "    \"\"\"\n",
      "    sorted_f = sort_freqs(freqs) if isinstance(freqs, dict) else freqs\n",
      "\n",
      "    # Don't show the tail\n",
      "    if isinstance(show, int):\n",
      "        # interpret as number of words to show in histogram\n",
      "        show_f = sorted_f[-show:]\n",
      "    else:\n",
      "        # interpret as a fraction\n",
      "        start = -int(round(show*len(freqs)))\n",
      "        show_f = sorted_f[start:]\n",
      "\n",
      "    # Now, extract words and counts, plot\n",
      "    n_words = len(show_f)\n",
      "    ind = np.arange(n_words)\n",
      "    words = [i[0] for i in show_f]\n",
      "    counts = [i[1] for i in show_f]\n",
      "\n",
      "    fig = plt.figure()\n",
      "    ax = fig.add_subplot(111)\n",
      "\n",
      "    if n_words<=20:\n",
      "        # Only show bars and x labels for small histograms, they don't make\n",
      "        # sense otherwise\n",
      "        ax.bar(ind, counts)\n",
      "        ax.set_xticks(ind)\n",
      "        ax.set_xticklabels(words, rotation=45)\n",
      "        fig.subplots_adjust(bottom=0.25)\n",
      "    else:\n",
      "        # For larger ones, do a step plot\n",
      "        ax.step(ind, counts)\n",
      "\n",
      "    # If it spans more than two decades, use a log scale\n",
      "    if float(max(counts))/min(counts) > 100:\n",
      "        ax.set_yscale('log')\n",
      "\n",
      "    if title:\n",
      "        ax.set_title(title)\n",
      "    return ax\n",
      "\n",
      "\n",
      "def summarize_centrality(centrality):\n",
      "    c = centrality.items()\n",
      "    c.sort(key=lambda x:x[1], reverse=True)\n",
      "    print '\\nGraph centrality'\n",
      "    for node, cent in c:\n",
      "        print \"%15s: %.3g\" % (node, float(cent))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# problem 3, part 2 begins here\n",
      "plt.rc('figure', figsize=(8, 5))\n",
      "import networkx as nx\n",
      "from __future__ import unicode_literals\n",
      "import requests\n",
      "import json\n",
      "from requests_oauthlib import OAuth1\n",
      "from urlparse import parse_qs\n",
      "import urllib2\n",
      "\n",
      "\n",
      "REQUEST_TOKEN_URL = \"https://api.twitter.com/oauth/request_token\"\n",
      "AUTHORIZE_URL = \"https://api.twitter.com/oauth/authorize?oauth_token=\"\n",
      "ACCESS_TOKEN_URL = \"https://api.twitter.com/oauth/access_token\"\n",
      "\n",
      "CONSUMER_KEY = \"8TriMgJ23HLUKum4nvP11w\"\n",
      "CONSUMER_SECRET = \"KxKCfqnZJtlrdNej4nmmjzFP2AscJeVvwYVwe0Gzec\"\n",
      "\n",
      "OAUTH_TOKEN =   \"36091385-fMXAXw88wooIkgY27mWKK9SOU6aMwFf1tBORknafs\" #\"36091385-fMXAXw88wooIkgY27mWKK9SOU6aMwFf1tBORknafs\"\n",
      "OAUTH_TOKEN_SECRET = \"j4pURuuA7tfmlzesYpbgIZdYg2QJz7ReP8oG9kgAuA\" ## \"j4pURuuA7tfmlzesYpbgIZdYg2QJz7ReP8oG9kgAuA\"\n",
      "\n",
      "\n",
      "def setup_oauth():\n",
      "    \"\"\"Authorize your app via identifier.\"\"\"\n",
      "    # Request token\n",
      "    oauth = OAuth1(CONSUMER_KEY, client_secret=CONSUMER_SECRET)\n",
      "    r = requests.post(url=REQUEST_TOKEN_URL, auth=oauth)\n",
      "    credentials = parse_qs(r.content)\n",
      "\n",
      "    resource_owner_key = credentials.get('oauth_token')[0]\n",
      "    resource_owner_secret = credentials.get('oauth_token_secret')[0]\n",
      "\n",
      "    # Authorize\n",
      "    authorize_url = AUTHORIZE_URL + resource_owner_key\n",
      "    print 'Please go here and authorize: ' + authorize_url\n",
      "\n",
      "    verifier = raw_input('Please input the verifier: ')\n",
      "    oauth = OAuth1(CONSUMER_KEY,\n",
      "                   client_secret=CONSUMER_SECRET,\n",
      "                   resource_owner_key=resource_owner_key,\n",
      "                   resource_owner_secret=resource_owner_secret,\n",
      "                   verifier=verifier)\n",
      "\n",
      "    # Finally, Obtain the Access Token\n",
      "    r = requests.post(url=ACCESS_TOKEN_URL, auth=oauth)\n",
      "    credentials = parse_qs(r.content)\n",
      "    token = credentials.get('oauth_token')[0]\n",
      "    secret = credentials.get('oauth_token_secret')[0]\n",
      "\n",
      "    return token, secret\n",
      "\n",
      "\n",
      "def get_oauth():\n",
      "    oauth = OAuth1(CONSUMER_KEY,\n",
      "                client_secret=CONSUMER_SECRET,\n",
      "                resource_owner_key=OAUTH_TOKEN,\n",
      "                resource_owner_secret=OAUTH_TOKEN_SECRET)\n",
      "    return oauth\n",
      "\n",
      "if not OAUTH_TOKEN:\n",
      "    token, secret = setup_oauth()\n",
      "    print \"OAUTH_TOKEN: \" + token\n",
      "    print \"OAUTH_TOKEN_SECRET: \" + secret\n",
      "    print\n",
      "else:\n",
      "    oauth = get_oauth()\n",
      "    # get tweets related to 'MH370', the recently lost plane from Malaysian Airlines\n",
      "    quoteString = urllib2.quote('MH370') # #USFCA\n",
      "    response = requests.get(url=\"https://api.twitter.com/1.1/search/tweets.json?q=\" + quoteString, auth=oauth)\n",
      "    tweets = json.loads(response.content, strict=False)['statuses']\n",
      "\n",
      "lines = lines_cleanup([tweet['text'] for tweet in tweets])\n",
      "words = '\\n'.join(lines).split()\n",
      "wf = word_freq(words)\n",
      "sorted_wf = sort_freqs(wf)\n",
      "summarize_freq_hist(sorted_wf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of unique words: 135\n",
        "\n",
        "10 least frequent words:\n",
        "http://t.co/xsrpn\u2026 -> 1\n",
        "             tukar -> 1\n",
        "            banyak -> 1\n",
        "              four -> 1\n",
        "             still -> 1\n",
        "            diduga -> 1\n",
        "      http://t.co\u2026 -> 1\n",
        "              pray -> 1\n",
        "              send -> 1\n",
        "              wall -> 1\n",
        "\n",
        "10 most frequent words:\n",
        "            @bharianmy -> 2\n",
        "                flight -> 2\n",
        "         #prayformh370 -> 2\n",
        "http://t.co/je2hgxad4q -> 2\n",
        "               langsat -> 2\n",
        "                 after -> 2\n",
        "                 hours -> 3\n",
        "               pesawat -> 3\n",
        "                 mh370 -> 6\n",
        "                #mh370 -> 7\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# plot showing the frequencies of top 10 most frequent words\n",
      "n_words = 10\n",
      "plot_word_histogram(sorted_wf, n_words,\"Frequencies for %s most frequent words\" % n_words)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# plot showing the histogram of the entire word list\n",
      "plot_word_histogram(sorted_wf, 1.0, \"Frequencies for entire word list\")\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# displays co-occurrences for top 20 most popular words\n",
      "n_nodes = 20\n",
      "popular = sorted_wf[-n_nodes:]\n",
      "pop_words = [wc[0] for wc in popular]\n",
      "co_occur = co_occurrences(lines, pop_words)\n",
      "co_occur"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "{(u'#goedemorgen', u'#mh370'): 1,\n",
        " (u'#goedemorgen', u'#mh370hilang'): 0,\n",
        " (u'#goedemorgen', u'#prayformh370'): 0,\n",
        " (u'#goedemorgen', u'@bharianmy'): 0,\n",
        " (u'#goedemorgen', u'@jonostrower'): 0,\n",
        " (u'#goedemorgen', u'about'): 1,\n",
        " (u'#goedemorgen', u'after'): 0,\n",
        " (u'#goedemorgen', u'cari'): 0,\n",
        " (u'#goedemorgen', u'flight'): 1,\n",
        " (u'#goedemorgen', u'hours'): 0,\n",
        " (u'#goedemorgen', u'http://t.co/je2hgxad4q'): 0,\n",
        " (u'#goedemorgen', u'langsat'): 0,\n",
        " (u'#goedemorgen', u'malaysia'): 0,\n",
        " (u'#goedemorgen', u'mh370'): 1,\n",
        " (u'#goedemorgen', u'pesawat'): 0,\n",
        " (u'#goedemorgen', u'petunjuk'): 0,\n",
        " (u'#goedemorgen', u'tiada'): 0,\n",
        " (u'#mh370hilang', u'#mh370'): 0,\n",
        " (u'#mh370hilang', u'#prayformh370'): 0,\n",
        " (u'#mh370hilang', u'@bharianmy'): 2,\n",
        " (u'#mh370hilang', u'@jonostrower'): 0,\n",
        " (u'#mh370hilang', u'about'): 0,\n",
        " (u'#mh370hilang', u'after'): 0,\n",
        " (u'#mh370hilang', u'cari'): 0,\n",
        " (u'#mh370hilang', u'flight'): 0,\n",
        " (u'#mh370hilang', u'hours'): 0,\n",
        " (u'#mh370hilang', u'http://t.co/je2hgxad4q'): 2,\n",
        " (u'#mh370hilang', u'langsat'): 0,\n",
        " (u'#mh370hilang', u'malaysia'): 0,\n",
        " (u'#mh370hilang', u'mh370'): 2,\n",
        " (u'#mh370hilang', u'pesawat'): 2,\n",
        " (u'#mh370hilang', u'petunjuk'): 2,\n",
        " (u'#mh370hilang', u'tiada'): 2,\n",
        " (u'#prayformh370', u'#mh370'): 2,\n",
        " (u'#prayformh370', u'after'): 0,\n",
        " (u'#prayformh370', u'hours'): 1,\n",
        " (u'#prayformh370', u'http://t.co/je2hgxad4q'): 0,\n",
        " (u'#prayformh370', u'langsat'): 0,\n",
        " (u'#prayformh370', u'mh370'): 2,\n",
        " (u'#prayformh370', u'pesawat'): 0,\n",
        " (u'@bharianmy', u'#mh370'): 2,\n",
        " (u'@bharianmy', u'#prayformh370'): 0,\n",
        " (u'@bharianmy', u'after'): 0,\n",
        " (u'@bharianmy', u'flight'): 0,\n",
        " (u'@bharianmy', u'hours'): 0,\n",
        " (u'@bharianmy', u'http://t.co/je2hgxad4q'): 2,\n",
        " (u'@bharianmy', u'langsat'): 0,\n",
        " (u'@bharianmy', u'mh370'): 2,\n",
        " (u'@bharianmy', u'pesawat'): 2,\n",
        " (u'@jonostrower', u'#mh370'): 1,\n",
        " (u'@jonostrower', u'#prayformh370'): 0,\n",
        " (u'@jonostrower', u'@bharianmy'): 0,\n",
        " (u'@jonostrower', u'after'): 1,\n",
        " (u'@jonostrower', u'flight'): 1,\n",
        " (u'@jonostrower', u'hours'): 1,\n",
        " (u'@jonostrower', u'http://t.co/je2hgxad4q'): 0,\n",
        " (u'@jonostrower', u'langsat'): 0,\n",
        " (u'@jonostrower', u'mh370'): 1,\n",
        " (u'@jonostrower', u'pesawat'): 0,\n",
        " (u'about', u'#mh370'): 2,\n",
        " (u'about', u'#prayformh370'): 0,\n",
        " (u'about', u'@bharianmy'): 0,\n",
        " (u'about', u'@jonostrower'): 1,\n",
        " (u'about', u'after'): 1,\n",
        " (u'about', u'flight'): 2,\n",
        " (u'about', u'hours'): 1,\n",
        " (u'about', u'http://t.co/je2hgxad4q'): 0,\n",
        " (u'about', u'langsat'): 0,\n",
        " (u'about', u'mh370'): 2,\n",
        " (u'about', u'pesawat'): 0,\n",
        " (u'after', u'#mh370'): 2,\n",
        " (u'after', u'hours'): 2,\n",
        " (u'after', u'mh370'): 2,\n",
        " (u'after', u'pesawat'): 0,\n",
        " (u'cari', u'#mh370'): 0,\n",
        " (u'cari', u'#prayformh370'): 0,\n",
        " (u'cari', u'@bharianmy'): 0,\n",
        " (u'cari', u'@jonostrower'): 0,\n",
        " (u'cari', u'about'): 0,\n",
        " (u'cari', u'after'): 0,\n",
        " (u'cari', u'flight'): 0,\n",
        " (u'cari', u'hours'): 0,\n",
        " (u'cari', u'http://t.co/je2hgxad4q'): 0,\n",
        " (u'cari', u'langsat'): 1,\n",
        " (u'cari', u'mh370'): 2,\n",
        " (u'cari', u'pesawat'): 1,\n",
        " (u'flight', u'#mh370'): 2,\n",
        " (u'flight', u'#prayformh370'): 0,\n",
        " (u'flight', u'after'): 1,\n",
        " (u'flight', u'hours'): 1,\n",
        " (u'flight', u'http://t.co/je2hgxad4q'): 0,\n",
        " (u'flight', u'langsat'): 0,\n",
        " (u'flight', u'mh370'): 2,\n",
        " (u'flight', u'pesawat'): 0,\n",
        " (u'hours', u'#mh370'): 3,\n",
        " (u'hours', u'mh370'): 3,\n",
        " (u'hours', u'pesawat'): 0,\n",
        " (u'http://t.co/je2hgxad4q', u'#mh370'): 0,\n",
        " (u'http://t.co/je2hgxad4q', u'after'): 0,\n",
        " (u'http://t.co/je2hgxad4q', u'hours'): 0,\n",
        " (u'http://t.co/je2hgxad4q', u'langsat'): 0,\n",
        " (u'http://t.co/je2hgxad4q', u'mh370'): 2,\n",
        " (u'http://t.co/je2hgxad4q', u'pesawat'): 2,\n",
        " (u'langsat', u'#mh370'): 0,\n",
        " (u'langsat', u'after'): 0,\n",
        " (u'langsat', u'hours'): 0,\n",
        " (u'langsat', u'mh370'): 1,\n",
        " (u'langsat', u'pesawat'): 0,\n",
        " (u'malaysia', u'#mh370'): 1,\n",
        " (u'malaysia', u'#prayformh370'): 0,\n",
        " (u'malaysia', u'@bharianmy'): 0,\n",
        " (u'malaysia', u'@jonostrower'): 0,\n",
        " (u'malaysia', u'about'): 1,\n",
        " (u'malaysia', u'after'): 0,\n",
        " (u'malaysia', u'cari'): 1,\n",
        " (u'malaysia', u'flight'): 0,\n",
        " (u'malaysia', u'hours'): 1,\n",
        " (u'malaysia', u'http://t.co/je2hgxad4q'): 0,\n",
        " (u'malaysia', u'langsat'): 0,\n",
        " (u'malaysia', u'mh370'): 2,\n",
        " (u'malaysia', u'pesawat'): 1,\n",
        " (u'mh370', u'#mh370'): 4,\n",
        " (u'notion', u'#goedemorgen'): 0,\n",
        " (u'notion', u'#mh370'): 0,\n",
        " (u'notion', u'#mh370hilang'): 0,\n",
        " (u'notion', u'#prayformh370'): 0,\n",
        " (u'notion', u'@bharianmy'): 0,\n",
        " (u'notion', u'@jonostrower'): 1,\n",
        " (u'notion', u'about'): 0,\n",
        " (u'notion', u'after'): 0,\n",
        " (u'notion', u'cari'): 0,\n",
        " (u'notion', u'flight'): 0,\n",
        " (u'notion', u'hours'): 0,\n",
        " (u'notion', u'http://t.co/je2hgxad4q'): 0,\n",
        " (u'notion', u'langsat'): 0,\n",
        " (u'notion', u'malaysia'): 0,\n",
        " (u'notion', u'mh370'): 0,\n",
        " (u'notion', u'pesawat'): 0,\n",
        " (u'notion', u'petunjuk'): 0,\n",
        " (u'notion', u'terakhir'): 0,\n",
        " (u'notion', u'tiada'): 0,\n",
        " (u'pesawat', u'#mh370'): 2,\n",
        " (u'pesawat', u'mh370'): 3,\n",
        " (u'petunjuk', u'#mh370'): 2,\n",
        " (u'petunjuk', u'#prayformh370'): 0,\n",
        " (u'petunjuk', u'@bharianmy'): 2,\n",
        " (u'petunjuk', u'@jonostrower'): 0,\n",
        " (u'petunjuk', u'about'): 0,\n",
        " (u'petunjuk', u'after'): 0,\n",
        " (u'petunjuk', u'cari'): 0,\n",
        " (u'petunjuk', u'flight'): 0,\n",
        " (u'petunjuk', u'hours'): 0,\n",
        " (u'petunjuk', u'http://t.co/je2hgxad4q'): 2,\n",
        " (u'petunjuk', u'langsat'): 0,\n",
        " (u'petunjuk', u'malaysia'): 0,\n",
        " (u'petunjuk', u'mh370'): 2,\n",
        " (u'petunjuk', u'pesawat'): 2,\n",
        " (u'petunjuk', u'tiada'): 2,\n",
        " (u'terakhir', u'#goedemorgen'): 0,\n",
        " (u'terakhir', u'#mh370'): 0,\n",
        " (u'terakhir', u'#mh370hilang'): 0,\n",
        " (u'terakhir', u'#prayformh370'): 0,\n",
        " (u'terakhir', u'@bharianmy'): 0,\n",
        " (u'terakhir', u'@jonostrower'): 0,\n",
        " (u'terakhir', u'about'): 0,\n",
        " (u'terakhir', u'after'): 0,\n",
        " (u'terakhir', u'cari'): 0,\n",
        " (u'terakhir', u'flight'): 0,\n",
        " (u'terakhir', u'hours'): 0,\n",
        " (u'terakhir', u'http://t.co/je2hgxad4q'): 0,\n",
        " (u'terakhir', u'langsat'): 0,\n",
        " (u'terakhir', u'malaysia'): 0,\n",
        " (u'terakhir', u'mh370'): 1,\n",
        " (u'terakhir', u'pesawat'): 0,\n",
        " (u'terakhir', u'petunjuk'): 0,\n",
        " (u'terakhir', u'tiada'): 0,\n",
        " (u'tiada', u'#mh370'): 2,\n",
        " (u'tiada', u'#prayformh370'): 0,\n",
        " (u'tiada', u'@bharianmy'): 2,\n",
        " (u'tiada', u'@jonostrower'): 0,\n",
        " (u'tiada', u'about'): 0,\n",
        " (u'tiada', u'after'): 0,\n",
        " (u'tiada', u'cari'): 0,\n",
        " (u'tiada', u'flight'): 0,\n",
        " (u'tiada', u'hours'): 0,\n",
        " (u'tiada', u'http://t.co/je2hgxad4q'): 2,\n",
        " (u'tiada', u'langsat'): 0,\n",
        " (u'tiada', u'malaysia'): 0,\n",
        " (u'tiada', u'mh370'): 2,\n",
        " (u'tiada', u'pesawat'): 2}"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wgraph = co_occurrences_graph(popular, co_occur, cutoff=0)\n",
      "wgraph = nx.connected_component_subgraphs(wgraph)[0]\n",
      "centrality = nx.eigenvector_centrality_numpy(wgraph)\n",
      "summarize_centrality(centrality)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Graph centrality\n",
        "          mh370: 0.477\n",
        "         #mh370: 0.398\n",
        "        pesawat: 0.31\n",
        "     @bharianmy: 0.271\n",
        "          tiada: 0.271\n",
        "       petunjuk: 0.271\n",
        "   #mh370hilang: 0.227\n",
        "http://t.co/je2hgxad4q: 0.227\n",
        "          hours: 0.227\n",
        "          about: 0.173\n",
        "         flight: 0.165\n",
        "          after: 0.164\n",
        "       malaysia: 0.134\n",
        "  #prayformh370: 0.123\n",
        "   @jonostrower: 0.1\n",
        "           cari: 0.0891\n",
        "   #goedemorgen: 0.0754\n",
        "        langsat: 0.0352\n",
        "       terakhir: 0.0297\n",
        "         notion: 0.00622\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# show the centrality and term co-occurrence graph\n",
      "query = \"MH370\"\n",
      "print \"Graph visualization for query:\", query\n",
      "plot_graph(wgraph, centrality_layout(wgraph, centrality), plt.figure(figsize=(8,8)),\n",
      "    title='Centrality and term co-occurrence graph, q=\"%s\"' % query)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Graph visualization for query: MH370\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# write the graphml file to be exported to gephi\n",
      "# the path will need to be changed as this is specific to the machine\n",
      "nx.write_graphml(wgraph, '/home/patthebug/Downloads/MH370.graphml')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    }
   ],
   "metadata": {}
  }
 ]
}